spring:
  ai:
    ollama:
#   if the app is running in a docker container, set the OLLAMA_BASE_URL to http://host.docker.internal:11434 or the http://rag-ollama:11434
      base-url: ${OLLAMA_BASE_URL:http://localhost:11434}
      chat:
        options:
          model: ${OLLAMA_CHAT_MODEL:llama3}
          temperature: ${OLLAMA_TEMPERATURE:0.2}
          max-tokens: ${OLLAMA_MAX_TOKENS:512}
      embedding:
        options:
          model: ${OLLAMA_EMBED_MODEL:nomic-embed-text}
    client:
      connect-timeout: 10s
      read-timeout: 10s
    retry:
      on-http-statuses: 429,500,502,503,504
