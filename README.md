# RAG Chat Storage

A production-ready microservice to store chat histories generated by a RAG-based chatbot system.

## Features
- Manage chat sessions (create, rename, favorite, delete)
- Store messages with sender, content, and optional context
- Pagination for messages
- API Key authentication (header: `X-API-KEY`), supports multiple keys via comma-separated list
- Simple rate limiting per API key / IP
- Global error handling and centralized logging baseline
- Health checks via Spring Boot Actuator
- OpenAPI/Swagger UI
- Centralized logging with Logstash + Elasticsearch + Kibana (ELK stack)
- Dockerized with MySQL, Adminer, ELK stack and the backend Spring Boot app

## Tech Stack
- Java 21, Spring Boot 3
- Spring Web, Spring Data JPA, MySQL
- Springdoc OpenAPI, Actuator

## Getting Started

### Prerequisites
- Docker and Docker Compose

### Environment Variables
Copy `.env.example` to `.env` and adjust as needed.

```
cp .env.example .env
```

- Local runs: .env is automatically loaded at startup (via spring-dotenv), so you don‚Äôt need to export variables manually.
- Docker Compose: docker compose automatically reads .env in the project root; the app service is configured with `env_file: .env`.

Key variables:
- `API_KEY` ‚Äì required to access APIs
- `DB_URL`, `DB_USERNAME`, `DB_PASSWORD` ‚Äì MySQL connection

### Run with Docker Compose
```
docker compose up --build
```
- Backend: http://localhost:8080 (container: rag-backend)
- UI: http://localhost:8081 (container: rag-ui)
- Adminer: http://localhost:8082 (container: rag-adminer)
  - Adminer connection to MySQL:
    - System: MySQL
    - Server: mysql (service hostname inside Compose network) or rag-mysql (container name)
    - User: root
    - Password: value of MYSQL_ROOT_PASSWORD in your .env (default: secret)
  - If you see "Connection refused" in Adminer:
    - Wait a few seconds; MySQL has a healthcheck and Adminer will start after it‚Äôs healthy.
    - Ensure you connect to Server: mysql (service) or rag-mysql (container), not localhost.
    - Check that host port 3306 isn‚Äôt already used by a local MySQL. If in use, edit docker-compose.yml and change the mapping to "3307:3306", then restart.
- MySQL (container name: rag-mysql) is available on host port 3306 by default.
- Health: http://localhost:8080/actuator/health (details enabled)
  - Liveness: http://localhost:8080/actuator/health/liveness
  - Readiness: http://localhost:8080/actuator/health/readiness
- Swagger UI: http://localhost:8080/swagger-ui/index.html
- Docs shortcut: http://localhost:8080/docs
- UI (separate Spring Boot): http://localhost:8081/ui (defaults to userId=demo if you hit /ui)
- Friendly UI error pages for not found (404) and general errors

Add header `X-API-KEY: <your api key>` to API requests. You can configure a single key via `API_KEY` or multiple keys via `API_KEYS` (comma-separated).

Troubleshooting 401 Unauthorized:
- Ensure you set API_KEY in your .env and restarted the app. The default is `changeme`.
- Send header name exactly as configured: by default `X-API-KEY`. You can change the header name with `security.api-key.header`.
- Using the included .http files, define @API_KEY or http-client.env.json with API_KEY.
- Public endpoints (no key required): `/`, `/actuator/health`, `/docs`, `/swagger-ui/**`, `/ui/**`, and static assets (`/css/**`, `/js/**`, `/images/**`, `/webjars/**`).

## API

Base path: `/api/v1`

### Users
- POST `/api/v1/users` ‚Äì create (or ensure) a user
  - Body: `{ "userId": "u1" }`
- GET `/api/v1/users` ‚Äì list users

### Sessions
- POST `/api/v1/sessions` ‚Äì create session
  - Body: `{ "userId": "u1", "title": "My chat" }`
- GET `/api/v1/sessions?userId=u1&favorite=true|false&page=0&size=20&q=term` ‚Äì list sessions for user (paginated, optional favorite, optional `q` for title contains; ordered by updatedAt desc)
- PATCH `/api/v1/sessions/{id}/title` ‚Äì rename session
  - Body: `{ "title": "New title" }`
- PATCH `/api/v1/sessions/{id}/favorite` ‚Äì mark/unmark favorite
  - Body: `{ "favorite": true }`
- DELETE `/api/v1/sessions/{id}` ‚Äì delete session and messages

### Messages
- POST `/api/v1/sessions/{id}/messages` ‚Äì add message
  - Body: `{ "sender": "USER|ASSISTANT|SYSTEM", "content": "...", "context": "..." }`
- GET `/api/v1/sessions/{id}/messages?page=0&size=20` ‚Äì list messages (paginated)

## UI

A separate Spring Boot UI (ui-app) is available at http://localhost:8081/ui with a ChatGPT-like dark theme and responsive layout:
- Left sidebar lists sessions with small, friendly controls per session (favorite ‚≠ê/‚òÜ, inline rename ‚úé, delete üóë), and a filter toggle (All / Favorites)
- Main chat area shows messages with alternating bubbles
- Sticky composer at the bottom to send USER messages quickly
- Now, when you send a message, the server calls OpenAI via Spring AI and posts the assistant reply back into the same session (requires OPENAI_API_KEY set). If the AI is unavailable or misconfigured, the UI shows a subtle notice instead of an error in the chat.

Tip: Open http://localhost:8080/ui to get redirected to a demo user (userId=demo).

Mobile: On small screens, use the ‚ò∞ button in the top bar to open/close the sessions sidebar.

## Correlating logs via X-Request-Id

This service emits structured JSON logs (Logback + logstash encoder). Each request receives a requestId that is:
- Accepted from the incoming header X-Request-Id, or generated if absent
- Added to MDC as requestId and returned as X-Request-Id response header
- Included in all log events for that request, including a single access log line with method, path, status, and duration

To trace a request end-to-end:
- Send a header: X-Request-Id: <your-id> (optional)
- Or take the X-Request-Id from the response header
- Filter your logs by {"requestId":"..."}

Example curl:

curl -i http://localhost:8080/api/v1/sessions?userId=demo \\
  -H "X-API-KEY: $API_KEY" \\
  -H "X-Request-Id: demo-req-123"

You will see one access log entry with that requestId.

## Profiles

- default: production-like settings reading environment variables.
- dev: convenient defaults for local development (enable H2 if configured, verbose logging). Run with:
  ./mvnw spring-boot:run -Dspring-boot.run.profiles=dev

### AI Provider profiles (runtime switchable)

The service can run against multiple LLM providers by selecting a Spring profile at runtime. If no profile is set, OpenAI is used by default.

Supported profiles:
- openai (default)
- anthropic (Claude for chat; OpenAI for embeddings)
- ollama (local models)
- openai-compatible (e.g., DeepSeek, Mistral servers that speak OpenAI API)

Select a provider at runtime:
- Locally: ./mvnw spring-boot:run -Dspring-boot.run.profiles=anthropic
- With env: SPRING_PROFILES_ACTIVE=ollama java -jar app.jar
- Docker/K8s: set SPRING_PROFILES_ACTIVE in the container/pod env

Readiness health includes the AI provider. If the provider is misconfigured/unreachable, readiness is DOWN.

## Production hardening checklist

- Run behind TLS (terminate HTTPS at your ingress or gateway).
- Rotate API keys regularly; prefer multiple keys via API_KEYS.
- Adjust rate limiting (rate-limit.requests-per-minute, burst) to your traffic.
- Set strict CORS allowed-origins for your frontend domains only.
- Set request size limits (MAX_FILE_SIZE, MAX_REQUEST_SIZE, request.max-bytes) per your needs.
- Configure persistent DB and Liquibase changelog; disable hibernate DDL (already disabled).
- Set provider env vars if using AI endpoints; readiness group includes ai.
- Observe resilience: retries and circuit breaker are configured for AI calls.
- Run the container as non-root (Dockerfile already does); set resource limits and replicas.
- Expose actuator endpoints carefully (currently only health, info).

## Development

Build & test:
```
mvn clean package
```

Run locally:
```
cp .env.example .env   # first time only
./mvnw spring-boot:run -Dspring-boot.run.profiles=dev
```

## Notes
- Security headers are set on all responses: X-Content-Type-Options=nosniff, X-Frame-Options=DENY, Referrer-Policy=no-referrer, and a restrictive Content-Security-Policy for the built-in UI (default-src 'self'; img-src 'self' data:; style-src 'self' 'unsafe-inline'; script-src 'self').
- Request size limits: set multipart limits via MAX_FILE_SIZE and MAX_REQUEST_SIZE (defaults 2MB). For non-multipart requests, you can set `request.max-bytes` to enforce a 413 on large payloads. 
- CORS is configurable via environment vars.
- Rate limiter uses a simple in-memory fixed-window; for production consider Redis or Bucket4j.


## AI (Spring AI)

This service integrates Spring AI for chat inference and embeddings using Spring AI abstractions (ChatClient, EmbeddingModel). Provider-specific setup is isolated to configuration classes and profile-specific properties.

Profiles and env vars:
- openai (default if no profile):
  - OPENAI_API_KEY, OPENAI_BASE_URL (optional)
  - OPENAI_CHAT_MODEL, OPENAI_EMBED_MODEL
- anthropic:
  - ANTHROPIC_API_KEY, ANTHROPIC_CHAT_MODEL
  - For embeddings uses OpenAI vars above
- ollama:
  - OLLAMA_BASE_URL, OLLAMA_CHAT_MODEL, OLLAMA_EMBED_MODEL
- openai-compatible (e.g., DeepSeek):
  - PROVIDER_API_KEY, PROVIDER_BASE_URL (and PROVIDER_*_MODEL)

Examples:
- Default OpenAI: SPRING_PROFILES_ACTIVE=openai OPENAI_API_KEY=sk-... ./mvnw spring-boot:run
- Anthropic: SPRING_PROFILES_ACTIVE=anthropic ANTHROPIC_API_KEY=... OPENAI_API_KEY=... ./mvnw spring-boot:run
- Ollama: SPRING_PROFILES_ACTIVE=ollama OLLAMA_BASE_URL=http://localhost:11434 ./mvnw spring-boot:run
- OpenAI-compatible: SPRING_PROFILES_ACTIVE=openai-compatible PROVIDER_API_KEY=... PROVIDER_BASE_URL=... ./mvnw spring-boot:run

### Endpoints
- POST /api/v1/ai/infer
  - Body: { "prompt": "Hello", "system": "You are a helpful assistant." }
  - Response: { "content": "...", "metadata": { ... } }
- POST /api/v1/ai/embeddings
  - Body: { "inputs": ["text 1", "text 2"] }
  - Response: { "data": [{"vector": [..]}, ...], "dimensions": 1536 }

Note: These endpoints require the API key header like other APIs (X-API-KEY).

## Centralized Logging (ELK)

This service supports centralized logging out of the box:

- Logs are emitted as structured JSON using Logback + logstash encoder.
- A Logstash service (see `docker-compose.yml`) forwards logs to Elasticsearch.
- Elasticsearch stores logs with full-text search and structured filtering.
- Kibana provides a web UI to explore, filter, and visualize logs in real time.

To access Kibana locally:
- Kibana: http://localhost:5601
- Elasticsearch: http://localhost:9200
- Logstash pipeline is defined in `logstash/pipeline/logstash.conf`

Authentication:
- If your Elasticsearch is secured, configure credentials in `.env`:
    * `ELASTIC_USER`
    * `ELASTIC_PASSWORD`

With centralized logging, you can:
- Search logs by `requestId` across distributed services.
- Monitor error rates, latency, and traffic patterns in Kibana dashboards.
- Set up alerts based on log events.

## Contributing

Please see CONTRIBUTING.md for guidelines.

Important: after each prompt that asks you to make changes, commit the changes when all is well (build passes and tests are green). Do not push; maintainers/CI will handle pushing as appropriate.
